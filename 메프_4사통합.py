# -*- coding: utf-8 -*-
"""메프_4사통합

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_78vb-7tYBw1doAd-ORAmfcpjMSDDhUC

### 방법론 요약
1. SM 주가+뉴스 기사 크롤링 데이터를 FinBERT로 전처리하여 감성분석 -> Sentiment Cache 데이터 생성
2. TemporalFusionTransformer 입력으로 넣기 위해 sentiment_cache 데이터를 TimeSeries 형태로 전처리
3. TemporalFusionTransformer 불러오기 & 훈련/검증/테스트 클래스 구축 & 손실함수(MSELoss)/옵티마이저 정의(AdamW)
4. 모델 훈련(10 epoch)
5. tensorboard로 손실 추이 확인
6. 모델 테스트 및 시각화

### 실행 환경 구축
"""

!pip install pytorch-forecasting pytorch-lightning

!pip install torchmetrics

import pandas as pd
import numpy as np
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import torch
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from torch import nn
import os
from tqdm import tqdm
import time
from pytorch_forecasting import TimeSeriesDataSet
from pytorch_forecasting.models import TemporalFusionTransformer
from pytorch_lightning import Trainer
from pytorch_lightning import LightningDataModule
from torchmetrics import MeanSquaredError
from pytorch_lightning import LightningModule
from pytorch_lightning.loggers import TensorBoardLogger

"""### 데이터 처리

"""

#FinBert를 활용한 감성분석. 최근 4사 데이터에 대해 따로 진행

'''
class StockDataProcessor:
    def __init__(self, file_path, tokenizer_model='yiyanghkust/finbert-tone', max_len=128, cache_path="sentiment_cache.csv"):
        self.file_path = file_path
        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_model)
        self.max_len = max_len
        self.data = None
        self.scaler = StandardScaler()
        self.cache_path = cache_path  # Path for cached data

    def load_data(self):
        """Load the CSV file."""
        self.data = pd.read_csv(self.file_path)
        print("Data loaded successfully.")
        return self

    def preprocess_data(self):
        """Preprocess the loaded data."""
        self.data['Date'] = pd.to_datetime(self.data['Date'])
        self.data = self.data.sort_values('Date')
        self.data['Return'] = self.data['Close'].pct_change().fillna(0)
        print("Data preprocessing completed.")
        return self

    class HeadlineDataset(Dataset):
        def __init__(self, headlines, tokenizer, max_len=128):
            self.headlines = headlines
            self.tokenizer = tokenizer
            self.max_len = max_len

        def __len__(self):
            return len(self.headlines)

        def __getitem__(self, idx):
            text = self.headlines[idx]
            encoding = self.tokenizer.encode_plus(
                text,
                max_length=self.max_len,
                add_special_tokens=True,
                padding='max_length',
                truncation=True,
                return_tensors='pt',
            )
            return {
                'input_ids': encoding['input_ids'].flatten(),
                'attention_mask': encoding['attention_mask'].flatten(),
            }

    def get_sentiment_scores(self):
        """Compute sentiment scores using FinBERT for multiple headlines (Top1~Top10)."""
        if os.path.exists(self.cache_path):
            print(f"Loading cached sentiment data from {self.cache_path}")
            self.data = pd.read_csv(self.cache_path)
        else:
            print("No cached sentiment data found. Computing sentiment scores...")
            headline_cols = [col for col in self.data.columns if col.startswith('top')]
            self.data['CombinedHeadline'] = self.data[headline_cols].fillna('').apply(' '.join, axis=1)

            dataset = self.HeadlineDataset(self.data['CombinedHeadline'], self.tokenizer, self.max_len)
            dataloader = DataLoader(dataset, batch_size=32, shuffle=False)

            model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')
            model.eval()

            sentiments = []
            with torch.no_grad():
                for batch in dataloader:
                    outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])
                    scores = torch.softmax(outputs.logits, dim=1).numpy()
                    sentiments.extend(scores)

            self.data['Sentiment'] = np.array(sentiments)[:, 1]
            self.data.to_csv(self.cache_path, index=False)  # Save the result for future use
            print(f"Sentiment analysis completed and cached at {self.cache_path}.")
        return self

    def engineer_features(self):
        """Engineer features for the model."""
        features = ['Open', 'High', 'Low', 'Close', 'Volume', 'Return', 'Sentiment']
        self.data = self.data.fillna(0)
        self.X = self.data[features].values
        self.y = self.data['Close'].shift(-1).fillna(method='ffill').values
        self.X = self.scaler.fit_transform(self.X)
        print("Feature engineering completed.")
        return self

    def split_data(self, val_size=0.1, test_size=0.2, random_state=42):
        """Split the data into train, validation, and test sets."""
        X_train, X_temp, y_train, y_temp = train_test_split(self.X, self.y, test_size=test_size + val_size, random_state=random_state)
        test_ratio = test_size / (test_size + val_size)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio, random_state=random_state)
        print("Data split into train, validation, and test sets.")
        return X_train, X_val, X_test, y_train, y_val, y_test
'''

class CachedDataProcessor:
    def __init__(self, company_name, sequence_length=30):
        self.company_name = company_name
        self.sequence_length = sequence_length
        self.data = None
        self.scaler = StandardScaler()

    def load_data(self):
        # 기업 데이터 로드
        self.data = pd.read_csv(f'{self.company_name}_data.csv')

        # 환율 데이터 로드 및 공백 제거
        ratio_df = pd.read_csv('ratio_formatted.csv')
        ratio_df.columns = ratio_df.columns.str.strip()  # 칼럼명의 공백 제거

        # 날짜 변환 및 병합
        self.data['date'] = pd.to_datetime(self.data['date'], format='%Y%m%d')
        ratio_df['date'] = pd.to_datetime(ratio_df['date'], format='%Y%m%d')
        self.data = pd.merge(self.data, ratio_df, on='date', how='left')

        return self

    def preprocess_data(self):
        self.data["time_idx"] = (self.data["date"] - self.data["date"].min()).dt.days
        self.data = self.data.fillna(0)
        self.data["close"] = self.scaler.fit_transform(self.data[["close"]])
        self.data["stock_id"] = self.company_name
        return self

    def prepare_time_series_dataset(self):
        self.dataset = TimeSeriesDataSet(
            self.data,
            time_idx="time_idx",
            target="close",
            group_ids=["stock_id"],
            max_encoder_length=self.sequence_length,
            max_prediction_length=1,
            time_varying_known_reals=["open", "high", "low", "volume", "change",
                                    "sentiment_average", "issue", "rank_change", "currency_rate"],
            time_varying_unknown_reals=["close"],
            static_categoricals=["stock_id"],
            allow_missing_timesteps=True
        )
        return self

    def split_time_series_dataset(self, val_size=0.1, test_size=0.2):
        total_length = len(self.dataset.index)
        train_cutoff = int((1 - val_size - test_size) * total_length)
        val_cutoff = int((1 - test_size) * total_length)

        indices = {
            'train': self.dataset.index.iloc[:train_cutoff],
            'val': self.dataset.index.iloc[train_cutoff:val_cutoff],
            'test': self.dataset.index.iloc[val_cutoff:]
        }

        datasets = {}
        for name, idx in indices.items():
            datasets[name] = TimeSeriesDataSet(
                self.data.iloc[idx["index_start"].min():idx["index_end"].max()],
                time_idx="time_idx",
                target="close",
                group_ids=["stock_id"],
                max_encoder_length=self.sequence_length,
                max_prediction_length=1,
                time_varying_known_reals=["open", "high", "low", "volume", "change",
                                        "sentiment_average", "issue", "rank_change", "currency_rate"],
                time_varying_unknown_reals=["close"],
                static_categoricals=["stock_id"],
                allow_missing_timesteps=True
            )

        return datasets['train'], datasets['val'], datasets['test']

#테스트용
'''
processor = StockDataProcessor('/content/YG_stock_and_headline.csv')

X_train, X_val, X_test, y_train, y_val, y_test = (
    processor.load_data()
             .preprocess_data()
             .get_sentiment_scores()
             .engineer_features()
             .split_data(val_size=0.1, test_size=0.2)
)
'''

"""### 모델 정의"""

class TimeSeriesDataModule(LightningDataModule):
    def __init__(self, train_dataset, val_dataset, batch_size=64, num_workers=2):
        super().__init__()
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.batch_size = batch_size
        self.num_workers = num_workers

    def train_dataloader(self):
        return self.train_dataset.to_dataloader(train=True, batch_size=self.batch_size, num_workers=self.num_workers)

    def val_dataloader(self):
        return self.val_dataset.to_dataloader(train=False, batch_size=self.batch_size, num_workers=self.num_workers)

class TFTLightningModule(LightningModule):
    def __init__(self, tft_model, loss, company_name):
        super().__init__()
        self.model = tft_model
        self.loss = loss
        self.company_name = company_name
        self.y_true = []
        self.y_pred = []

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        if isinstance(y, tuple):
            y = y[0]
        output = self(x)
        y_pred = output["prediction"].squeeze(-1)
        loss = self.loss(y_pred, y)
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        if isinstance(y, tuple):
            y = y[0]
        output = self(x)
        y_pred = output["prediction"].squeeze(-1)
        loss = self.loss(y_pred, y)
        self.log("val_loss", loss)
        return loss

    def test_step(self, batch, batch_idx):
        x, y = batch
        if isinstance(y, tuple):
            y = y[0]
        output = self(x)
        y_pred = output["prediction"].squeeze(-1)
        loss = self.loss(y_pred, y)
        self.log("test_loss", loss)
        self.y_true.append(y.cpu())
        self.y_pred.append(y_pred.cpu())
        return {"test_loss": loss}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.model.hparams.learning_rate)

"""###모델 훈련(구)"""

'''
datamodule = TimeSeriesDataModule(
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    batch_size=64,
    num_workers=2,
)

'''
logger = TensorBoardLogger("lightning_logs", name="tft_model")
'''

'''
trainer = Trainer(
    max_epochs=10,
    accelerator="gpu" if torch.cuda.is_available() else "cpu",
    devices=1 if torch.cuda.is_available() else 1,
    logger=logger
)
'''

'''
tft_model = TemporalFusionTransformer.from_dataset(
    processor.train_dataset,
    learning_rate=1e-3,
    hidden_size=64,
    attention_head_size=4,
    dropout=0.1,
    logging_metrics=None,
    output_size=1,
)
'''

#model = TFTLightningModule(tft_model, loss=MeanSquaredError())

#trainer.fit(model, datamodule=datamodule)

"""### 모델 훈련"""

companies = ['SM', 'YG', 'JYP', 'HYBE']
ratio_df = pd.read_csv('ratio_formatted.csv')
ratio_df['date'] = pd.to_datetime(ratio_df['date'], format='%Y%m%d')

results = {}

for company in companies:
    print(f"\nProcessing {company} data...")

    processor = CachedDataProcessor(company_name=company, sequence_length=30)

    processor.load_data()
    processor.preprocess_data()
    processor.prepare_time_series_dataset()

    # 데이터셋 분할 결과를 processor에 저장
    processor.train_dataset, processor.val_dataset, processor.test_dataset = processor.split_time_series_dataset(
        val_size=0.1,
        test_size=0.2
    )

    datamodule = TimeSeriesDataModule(
        train_dataset=processor.train_dataset,
        val_dataset=processor.val_dataset,
        batch_size=64,
        num_workers=2
    )

    logger = TensorBoardLogger("lightning_logs", name=f"{company}_model")

    trainer = Trainer(
        max_epochs=10,
        accelerator="gpu" if torch.cuda.is_available() else "cpu",
        devices=1,
        logger=logger
    )

    tft = TemporalFusionTransformer.from_dataset(
        processor.train_dataset,
        learning_rate=1e-3,
        hidden_size=64,
        attention_head_size=4,
        dropout=0.1,
        output_size=1
    )

    model = TFTLightningModule(tft, loss=MeanSquaredError(), company_name=company)
    trainer.fit(model, datamodule=datamodule)

    # 테스트 결과 저장
    test_dataloader = processor.test_dataset.to_dataloader(train=False, batch_size=64)
    test_results = trainer.test(model, dataloaders=test_dataloader)

    results[company] = {
        'y_true': torch.cat(model.y_true, dim=0).numpy(),
        'y_pred': torch.cat(model.y_pred, dim=0).numpy(),
        'scaler': processor.scaler,
        'processor': processor,  # processor 추가
    }

"""### 손실 확인"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir lightning_logs/

print("Final training loss:", trainer.callback_metrics["train_loss"])
print("Final validation loss:", trainer.callback_metrics["val_loss"])

"""### 모델 테스트 및 시각화"""

'''
test_dataloader = processor.test_dataset.to_dataloader(train=False, batch_size=64, num_workers=2)

test_results = trainer.test(model, dataloaders=test_dataloader)
'''

# 각 회사별 결과 시각화
for company, result in results.items():
    y_true_unscaled = result['scaler'].inverse_transform(result['y_true'].reshape(-1, 1))
    y_pred_unscaled = result['scaler'].inverse_transform(result['y_pred'].reshape(-1, 1))

    plt.figure(figsize=(10, 6))
    plt.plot(y_true_unscaled, label="actual")
    plt.plot(y_pred_unscaled, label="prediction")
    plt.title(f"{company} stock prediction result")
    plt.xlabel("time")
    plt.ylabel("price")
    plt.legend()
    plt.show()

!sudo apt-get install -y fonts-nanum

"""###결과 분석 및 시각화"""

from matplotlib import rc
import re

# 나눔고딕 폰트 설정
plt.rc('font', family='NanumBarunGothic')
plt.rcParams['axes.unicode_minus'] =False

# 필요한 라이브러리 import
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
import matplotlib.pyplot as plt
import itertools


# 데이터 형태 출력으로 디버깅
print("y_true shape:", y_true.shape)
print("y_pred shape:", y_pred.shape)
print("y_true dtype:", y_true.dtype)
print("y_pred dtype:", y_pred.dtype)

# 방향성 분석
def analyze_direction_accuracy(y_true, y_pred):
    # numpy array로 변환
    y_true = np.array(y_true).flatten()
    y_pred = np.array(y_pred).flatten()

    # 방향성 계산 (다음날 가격이 오늘보다 올랐는지)
    true_direction = np.diff(y_true) > 0
    pred_direction = np.diff(y_pred) > 0

    # bool을 int로 변환 (scikit-learn 요구사항)
    true_direction = true_direction.astype(int)
    pred_direction = pred_direction.astype(int)

    # 빈 배열 체크
    if len(true_direction) == 0 or len(pred_direction) == 0:
        print("Warning: Empty array detected in direction analysis")
        return None, None, None

    # 방향성 예측 정확도
    accuracy = np.mean(true_direction == pred_direction)

    # 혼동 행렬 계산
    conf_matrix = confusion_matrix(true_direction, pred_direction)

    # 상세 메트릭스 계산
    report = classification_report(true_direction, pred_direction,
                                 target_names=['하락', '상승'],
                                 digits=4)

    return accuracy, conf_matrix, report, true_direction, pred_direction

# 데이터가 비어있지 않은지 확인
if len(y_true) > 1 and len(y_pred) > 1:  # diff를 위해 최소 2개의 데이터 필요
    # 방향성 분석 실행
    accuracy, conf_matrix, report, true_direction, pred_direction = analyze_direction_accuracy(y_true, y_pred)

    if accuracy is not None:  # 분석이 성공적으로 완료된 경우에만 결과 출력
        # 결과 출력
        print("\n=== 주가 방향성 예측 분석 결과 ===")
        print(f"\n방향성 예측 정확도: {accuracy:.2%}")

        print("\n혼동 행렬:")
        print("예측↓ 실제→ [하락, 상승]")
        print(conf_matrix)

        print("\n상세 분석 리포트:")
        print(report)

        # 방향성 예측 시각화
        plt.figure(figsize=(12, 6))

        plt.subplot(2, 1, 1)
        plt.plot(true_direction, label='실제 방향', alpha=0.7)
        plt.plot(pred_direction, label='예측 방향', alpha=0.7)
        plt.title('주가 방향성: 실제 vs 예측')
        plt.legend()
        plt.grid(True)

        # 정확도 시각화 (이동 평균)
        window = 20  # 20일 이동 평균
        accuracy_ma = np.convolve(
            (true_direction == pred_direction).astype(float),
            np.ones(window)/window,
            mode='valid'
        )

        plt.subplot(2, 1, 2)
        plt.plot(accuracy_ma, label=f'{window}일 이동 평균 정확도', color='green')
        plt.axhline(y=accuracy, color='r', linestyle='--', label='전체 평균 정확도')
        plt.title('방향성 예측 정확도 추이')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.show()

        # 전체 정확도와 함께 각 방향별 정확도도 계산
        up_mask = true_direction == 1
        down_mask = true_direction == 0

        up_accuracy = np.mean(pred_direction[up_mask] == true_direction[up_mask]) if np.any(up_mask) else 0
        down_accuracy = np.mean(pred_direction[down_mask] == true_direction[down_mask]) if np.any(down_mask) else 0

        print("\n=== 추가 분석 ===")
        print(f"상승 예측 정확도: {up_accuracy:.2%}")
        print(f"하락 예측 정확도: {down_accuracy:.2%}")

        # 연속 정확도 분석
        correct_predictions = (true_direction == pred_direction).astype(int)
        consecutive_correct = np.array([len(list(group)) for key, group in itertools.groupby(correct_predictions) if key == 1])
        if len(consecutive_correct) > 0:
            print(f"\n최대 연속 정확 예측 일수: {np.max(consecutive_correct)}")
            print(f"평균 연속 정확 예측 일수: {np.mean(consecutive_correct):.1f}")
else:
    print("Error: 분석을 위한 충분한 데이터가 없습니다.")

def analyze_predictions_for_company(company, results_dict):
    y_true = results_dict[company]['y_true']
    y_pred = results_dict[company]['y_pred']

    # 방향성 계산
    true_direction = np.diff(y_true.flatten()) > 0  # flatten 추가
    pred_direction = np.diff(y_pred.flatten()) > 0  # flatten 추가

    # 예측 정확도 계산
    accuracy = np.mean(true_direction == pred_direction)
    print(f"\n=== {company} 방향성 예측 정확도: {accuracy:.2%} ===")

    # 연속 정확 예측 분석
    correct_predictions = (true_direction == pred_direction).astype(int)
    consecutive_counts = []
    current_count = 0

    for pred in correct_predictions:
        if pred == 1:
            current_count += 1
        elif current_count > 0:
            consecutive_counts.append(current_count)
            current_count = 0

    if consecutive_counts:
        print(f"최대 연속 정확 예측: {max(consecutive_counts)}일")
        print(f"평균 연속 정확 예측: {np.mean(consecutive_counts):.1f}일")

# 사용
for company in companies:
    if company in results:
        analyze_predictions_for_company(company, results)

# 다음날 예측을 위한 데이터 준비
def predict_next_day(model, processor):
    test_dataloader = processor.test_dataset.to_dataloader(
        train=False,
        batch_size=1,
        num_workers=0
    )

    for batch in test_dataloader:
        last_x = batch[0]

    with torch.no_grad():
        output = model(last_x)
        next_day_pred = output["prediction"].cpu().numpy()
        print("Prediction shape:", next_day_pred.shape)
        return next_day_pred.flatten()[-1]


# 회사별 데이터 처리 루프
for company in companies:
    try:
        # 원본 데이터 가져오기
        original_data = pd.read_csv(f'{company}_data.csv')
        original_data['date'] = pd.to_datetime(original_data['date'], format='%Y%m%d')

        # 데이터 준비
        last_30_original = original_data['close'].iloc[-30:].copy()
        last_30_dates = original_data['date'].iloc[-30:]
        next_day_date = last_30_dates.iloc[-1] + pd.Timedelta(days=1)

        print("Last date:", last_30_dates.iloc[-1])

        # 예측 수행
        next_day_scaled = predict_next_day(model, processor)
        print("Predicted scaled value:", next_day_scaled)

        # 스케일링된 예측값을 원래 스케일로 변환
        next_day_price = processor.scaler.inverse_transform([[next_day_scaled]])[0][0]
        print("Predicted actual price:", next_day_price)

        # 시각화
        plt.figure(figsize=(15, 8))

        # 기본 스타일 설정
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.rcParams['axes.facecolor'] = '#f0f0f0'
        plt.rcParams['figure.facecolor'] = 'white'

        # 날짜 배열 생성
        dates = list(last_30_dates)
        dates.append(next_day_date)

        # 데이터 배열 생성 (원본 가격 사용)
        values = np.append(last_30_original.values, next_day_price)

        # 실제 데이터 플롯
        plt.plot(dates[:-1], values[:-1], 'b-', label='실제 종가', linewidth=2)

        # 예측값 플롯
        plt.plot(dates[-2:], values[-2:], 'r--', label='예측 종가', linewidth=2)
        plt.scatter(dates[-1], values[-1], color='red', s=100, zorder=5)

        # 그래프 꾸미기
        plt.title('종가 예측 (마지막 30일 + 다음날 예측)', fontsize=15, pad=20)
        plt.xlabel('날짜', fontsize=12)
        plt.ylabel('주가 (원)', fontsize=12)
        plt.legend(fontsize=10, loc='upper left')

        # x축 날짜 포맷 설정
        plt.gcf().autofmt_xdate()

        # 최소/최대값 설정으로 여백 조절
        ymin, ymax = values.min(), values.max()
        plt.ylim(ymin - (ymax - ymin) * 0.1, ymax + (ymax - ymin) * 0.1)

        # 예측값 주석 추가
        plt.annotate(f'예측가: {int(next_day_price):,}원',
                     xy=(dates[-1], values[-1]),
                     xytext=(10, 10), textcoords='offset points',
                     bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),
                     arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2'))

        # 가격 축에 천 단위 구분자 추가
        plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))

        plt.tight_layout()
        plt.show()

        # 예측 결과 출력
        print(f"\n=== 다음날 종가 예측 결과 ===")
        print(f"예측 날짜: {next_day_date.strftime('%Y-%m-%d')}")
        print(f"예측 종가: {int(next_day_price):,}원")

        # 전일 대비 등락률 계산
        last_day_price = values[-2]  # 마지막 실제 종가
        change_rate = ((next_day_price - last_day_price) / last_day_price) * 100

        print(f"\n=== 전일 대비 분석 ===")
        print(f"전일 종가: {int(last_day_price):,}원")
        print(f"등락률: {change_rate:+.2f}%")
        print(f"예상 방향: {'상승' if change_rate > 0 else '하락'}")

    except Exception as e:
        print("Error occurred:", e)
        import traceback
        traceback.print_exc()